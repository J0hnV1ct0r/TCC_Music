{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2e1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d11e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import  LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4cfe2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "548df272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e90fc0",
   "metadata": {},
   "source": [
    "# Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa18071",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_pt1 = pd.read_csv(\"Dados/spotify_youtube_up_pt1.csv\")\n",
    "spotify_youtube_pt2 = pd.read_csv(\"Dados/spotify_youtube_up_pt2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3c7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extracao = pd.to_datetime(\"2023-02-07\")\n",
    "spotify_youtube_pt1[\"upload_date\"] = pd.to_datetime(spotify_youtube_pt1[\"upload_date\"])\n",
    "spotify_youtube_pt1[\"dias_na_plataforma\"] = (data_extracao - spotify_youtube_pt1[\"upload_date\"]).dt.days\n",
    "\n",
    "spotify_youtube_pt2[\"upload_date\"] = pd.to_datetime(spotify_youtube_pt2[\"upload_date\"])\n",
    "spotify_youtube_pt2[\"dias_na_plataforma\"] = (data_extracao - spotify_youtube_pt2[\"upload_date\"]).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bceec3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube = pd.concat([spotify_youtube_pt1,spotify_youtube_pt2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7751227",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_dummies= pd.get_dummies(spotify_youtube,columns=['Album_type'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af26e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_dummies['artist_number'] = spotify_youtube_dummies['Artist'].str.split(',').str.len()\n",
    "spotify_youtube_dummies['engagement_rate'] = spotify_youtube_dummies.apply(lambda row: (row['Likes'] + row['Comments']) / row['Views'] * 100 if row['Views'] > 0 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18e7aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop=['Unnamed: 0', 'Artist', 'Url_spotify', 'Track', 'Album', 'Uri','Url_youtube', 'Title', 'Channel', 'Views', 'Likes','Comments', 'Description', 'Licensed', 'official_video', 'upload_date']\n",
    "spotify_youtube_df=spotify_youtube_dummies.drop(columns=columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e47b0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_limpo=spotify_youtube_df.dropna(subset=['Danceability','Energy','Key','Loudness','Speechiness','Acousticness', 'Instrumentalness','Liveness','Valence','Tempo','Duration_ms','Stream','engagement_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a93e8e",
   "metadata": {},
   "source": [
    "# Preparação dos dados para os experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01f1f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_30_df=spotify_youtube_limpo[spotify_youtube_limpo['dias_na_plataforma']<31]\n",
    "youtube_90_df=spotify_youtube_limpo[spotify_youtube_limpo['dias_na_plataforma']<91]\n",
    "youtube_365_df=spotify_youtube_limpo[spotify_youtube_limpo['dias_na_plataforma']<366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8728d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_df_366_more = spotify_youtube_limpo.drop(youtube_365_df.index)\n",
    "spotify_youtube_df_91_365 = youtube_365_df.drop(youtube_90_df.index)\n",
    "spotify_youtube_df_31_90 = youtube_365_df.drop(youtube_30_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f439271",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_30_df=youtube_30_df[youtube_30_df['engagement_rate']<9.338507725109846]\n",
    "spotify_youtube_df_31_90=spotify_youtube_df_31_90[spotify_youtube_df_31_90['engagement_rate']<4.602401258613792]\n",
    "spotify_youtube_df_91_365=spotify_youtube_df_91_365[spotify_youtube_df_91_365['engagement_rate']<4.253191569584401]\n",
    "spotify_youtube_df_366_more=spotify_youtube_df_366_more[spotify_youtube_df_366_more['engagement_rate']<2.251193229196198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e18910a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_30_df_x = youtube_30_df.drop('engagement_rate',axis=1)\n",
    "youtube_30_df_y = youtube_30_df['engagement_rate']\n",
    "\n",
    "spotify_youtube_df_31_90_x = spotify_youtube_df_31_90.drop('engagement_rate',axis=1)\n",
    "spotify_youtube_df_31_90_y = spotify_youtube_df_31_90['engagement_rate']\n",
    "\n",
    "spotify_youtube_df_91_365_x = spotify_youtube_df_91_365.drop('engagement_rate',axis=1)\n",
    "spotify_youtube_df_91_365_y = spotify_youtube_df_91_365['engagement_rate']\n",
    "\n",
    "spotify_youtube_df_366_more_x = spotify_youtube_df_366_more.drop('engagement_rate',axis=1)\n",
    "spotify_youtube_df_366_more_y = spotify_youtube_df_366_more['engagement_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95e9e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_youtube_30X_train_val, spotify_youtube_30X_test, spotify_youtube_30y_train_val, spotify_youtube_30y_test = train_test_split(youtube_30_df_x, youtube_30_df_y, test_size=0.25, random_state=42)\n",
    "spotify_youtube_df_31_90X_train, spotify_youtube_df_31_90X_test, spotify_youtube_df_31_90y_train, spotify_youtube_df_31_90y_test = train_test_split(spotify_youtube_df_31_90_x, spotify_youtube_df_31_90_y, test_size=0.25, random_state=42)\n",
    "spotify_youtube_df_91_365X_train, spotify_youtube_df_91_365X_test, spotify_youtube_df_91_365y_train, spotify_youtube_df_91_365y_test = train_test_split(spotify_youtube_df_91_365_x, spotify_youtube_df_91_365_y, test_size=0.25, random_state=42)\n",
    "spotify_youtube_30X_train, spotify_youtube_30X_val, spotify_youtube_30y_train, spotify_youtube_30y_val = train_test_split(spotify_youtube_30X_train_val, spotify_youtube_30y_train_val, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b775c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de treino \n",
    "sy_x_train=pd.concat([spotify_youtube_30X_train,spotify_youtube_df_31_90X_train,spotify_youtube_df_91_365X_train,spotify_youtube_df_366_more_x])\n",
    "sy_y_train=pd.concat([spotify_youtube_30y_train,spotify_youtube_df_31_90y_train,spotify_youtube_df_91_365y_train,spotify_youtube_df_366_more_y])\n",
    "\n",
    "# Normalização do treino\n",
    "scaler_all = MinMaxScaler()\n",
    "sy_x_train_scaled = scaler_all.fit_transform(sy_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b007bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização do treino de 30 dias\n",
    "sy_30_x_train_scaled = scaler_all.transform(spotify_youtube_30X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51d3e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para treinar o modelo com dados de 0 a 90 dias\n",
    "sy_90_x_train=pd.concat([spotify_youtube_30X_train,spotify_youtube_df_31_90X_train])\n",
    "sy_90_y_train=pd.concat([spotify_youtube_30y_train,spotify_youtube_df_31_90y_train])\n",
    "\n",
    "# Normalização do treino de 90 dias\n",
    "sy_90_x_train_scaled = scaler_all.transform(sy_90_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8d088d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para treinar o modelo com dados de 0 a 365 dias\n",
    "sy_365_x_train=pd.concat([spotify_youtube_30X_train,spotify_youtube_df_31_90X_train,spotify_youtube_df_91_365X_train])\n",
    "sy_365_y_train=pd.concat([spotify_youtube_30y_train,spotify_youtube_df_31_90y_train,spotify_youtube_df_91_365y_train])\n",
    "\n",
    "# Normalização do treino de 90 dias\n",
    "sy_365_x_train_scaled = scaler_all.transform(sy_365_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d672c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para testar o desempenho do modelo com videos com ate 30 dias\n",
    "sy_30_x_test=spotify_youtube_30X_test\n",
    "sy_30_y_test=spotify_youtube_30y_test\n",
    "\n",
    "# validação de 30 dias\n",
    "sy_30_x_val=spotify_youtube_30X_val\n",
    "sy_30_y_val=spotify_youtube_30y_val\n",
    "\n",
    "# Para testar o desempenho do modelo com videos com ate 90 dias\n",
    "sy_90_x_test=pd.concat([spotify_youtube_30X_test,spotify_youtube_df_31_90X_test])\n",
    "sy_90_y_test=pd.concat([spotify_youtube_30y_test,spotify_youtube_df_31_90y_test])\n",
    "\n",
    "# Para testar o desempenho do modelo com videos com ate 1 ano\n",
    "sy_365_x_test=pd.concat([spotify_youtube_30X_test,spotify_youtube_df_31_90X_test,spotify_youtube_df_91_365X_test])\n",
    "sy_365_y_test=pd.concat([spotify_youtube_30y_test,spotify_youtube_df_31_90y_test,spotify_youtube_df_91_365y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4946da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizção dos dados de teste de 30 dias\n",
    "sy_30_x_test_scaled = scaler_all.transform(sy_30_x_test)\n",
    "\n",
    "# Normalizção dos dados de teste de 90 dias\n",
    "sy_90_x_test_scaled = scaler_all.transform(sy_90_x_test)\n",
    "\n",
    "# Normalizção dos dados de teste de 365 dias\n",
    "sy_365_x_test_scaled = scaler_all.transform(sy_365_x_test)\n",
    "\n",
    "# Normalização dos dados de validação de 30 dias\n",
    "sy_30_x_val_scaled = scaler_all.transform(sy_30_x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea9d9d0",
   "metadata": {},
   "source": [
    "# Tuning dos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a76bf24",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb632b",
   "metadata": {},
   "source": [
    "Fazendo o tuning com GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "082d3c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 81 candidates, totalling 81 fits\n",
      "Mean Squared Error (mse) on test data: 3.030052694606655\n",
      "Modelo salvo em: Modelos\\best_random_forest_model_grid.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Faculdade\\TCC_Music\\TCC_Music\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definindo os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Inicializando o modelo Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Configurando o GridSearchCV com PredefinedSplit\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=ps,  # Passa o PredefinedSplit\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinando o GridSearchCV\n",
    "grid_search.fit(X_full, y_full)\n",
    "\n",
    "# Obtendo o melhor modelo\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_rf.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (mse) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_random_forest_model_grid.pkl')\n",
    "joblib.dump(best_rf, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfd575",
   "metadata": {},
   "source": [
    "Fazendo o tuning com Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7da90913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 15:48:12,418] A new study created in memory with name: no-name-962186c8-52a9-4712-bb9a-de1d56478ff4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 15:48:36,066] Trial 0 finished with value: 3.185152477846704 and parameters: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 3.185152477846704.\n",
      "[I 2025-04-17 15:49:15,952] Trial 1 finished with value: 3.2432106355455184 and parameters: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 3.185152477846704.\n",
      "[I 2025-04-17 15:49:29,811] Trial 2 finished with value: 3.1894746893594825 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 3.185152477846704.\n",
      "[I 2025-04-17 15:50:11,023] Trial 3 finished with value: 3.268680418278824 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 0 with value: 3.185152477846704.\n",
      "[I 2025-04-17 15:50:46,075] Trial 4 finished with value: 3.214158643073261 and parameters: {'n_estimators': 150, 'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 3.185152477846704.\n",
      "[I 2025-04-17 15:51:48,599] Trial 5 finished with value: 3.183311954730591 and parameters: {'n_estimators': 200, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 5 with value: 3.183311954730591.\n",
      "[I 2025-04-17 15:52:21,901] Trial 6 finished with value: 3.253525807064163 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 5 with value: 3.183311954730591.\n",
      "[I 2025-04-17 15:53:06,579] Trial 7 finished with value: 3.126437540617682 and parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 7 with value: 3.126437540617682.\n",
      "[I 2025-04-17 15:53:53,227] Trial 8 finished with value: 3.303128748770124 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 7 with value: 3.126437540617682.\n",
      "[I 2025-04-17 15:54:08,241] Trial 9 finished with value: 3.2512846295330555 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 7 with value: 3.126437540617682.\n",
      "[I 2025-04-17 15:54:23,071] Trial 10 finished with value: 2.942138845711254 and parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 2.942138845711254.\n",
      "[I 2025-04-17 15:54:36,869] Trial 11 finished with value: 2.942138845711254 and parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 10 with value: 2.942138845711254.\n",
      "[I 2025-04-17 15:54:49,139] Trial 12 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:55:00,269] Trial 13 finished with value: 3.0232810372478283 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:55:12,928] Trial 14 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:55:24,350] Trial 15 finished with value: 3.0232810372478283 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:55:43,748] Trial 16 finished with value: 3.305022340299911 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:55:59,041] Trial 17 finished with value: 2.9935422109163885 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:56:10,705] Trial 18 finished with value: 3.0096540928185207 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:56:25,125] Trial 19 finished with value: 3.1622959668255235 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:56:35,604] Trial 20 finished with value: 3.0096540928185207 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:56:50,001] Trial 21 finished with value: 2.942138845711254 and parameters: {'n_estimators': 50, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:57:04,249] Trial 22 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:57:27,817] Trial 23 finished with value: 3.1331311971683844 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:57:41,825] Trial 24 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:57:53,492] Trial 25 finished with value: 3.0232810372478283 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:58:18,875] Trial 26 finished with value: 3.1299996172965985 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:58:27,108] Trial 27 finished with value: 3.013507567465224 and parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:58:50,729] Trial 28 finished with value: 3.1331311971683844 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:59:13,743] Trial 29 finished with value: 3.2103967474258126 and parameters: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:59:26,172] Trial 30 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:59:38,574] Trial 31 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 15:59:51,773] Trial 32 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:00:03,925] Trial 33 finished with value: 2.9935422109163885 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:00:16,221] Trial 34 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:00:38,546] Trial 35 finished with value: 3.2402909849985058 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:00:50,900] Trial 36 finished with value: 2.9935422109163885 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:01:16,290] Trial 37 finished with value: 3.141650367376854 and parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:02:03,444] Trial 38 finished with value: 3.1797894065888768 and parameters: {'n_estimators': 150, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:02:16,781] Trial 39 finished with value: 3.0339518600914737 and parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:02:45,652] Trial 40 finished with value: 3.230824891976463 and parameters: {'n_estimators': 100, 'max_depth': 30, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:03:01,304] Trial 41 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:03:14,755] Trial 42 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:03:27,790] Trial 43 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:03:39,227] Trial 44 finished with value: 2.9935422109163885 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:03:49,847] Trial 45 finished with value: 3.333185837014767 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:04:02,670] Trial 46 finished with value: 3.0232810372478283 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:04:52,514] Trial 47 finished with value: 3.131412242328163 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:05:05,738] Trial 48 finished with value: 2.938938448650455 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 12 with value: 2.938938448650455.\n",
      "[I 2025-04-17 16:05:26,213] Trial 49 finished with value: 3.2363172093733557 and parameters: {'n_estimators': 100, 'max_depth': 20, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 12 with value: 2.938938448650455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "Mean Squared Error (MSE) on test data: 2.880698838055008\n",
      "Modelo salvo em: Modelos\\best_random_forest_model_optuna.pkl\n"
     ]
    }
   ],
   "source": [
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Função objetivo para o Optuna\n",
    "def objective(trial):\n",
    "    # Sugerindo os hiperparâmetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 30, step=10)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10, step=2)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 4, step=1)\n",
    "\n",
    "    # Criando o modelo com os hiperparâmetros sugeridos\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Treinando o modelo nos dados de treino\n",
    "    rf.fit(X_full[ps.test_fold == -1], y_full[ps.test_fold == -1])\n",
    "\n",
    "    # Avaliando o modelo nos dados de validação\n",
    "    predictions = rf.predict(X_full[ps.test_fold == 0])\n",
    "    mse = mean_squared_error(y_full[ps.test_fold == 0], predictions)\n",
    "\n",
    "    return mse  # O objetivo é minimizar o MSE\n",
    "\n",
    "# Criando o estudo do Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Número de tentativas\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = study.best_params\n",
    "print(\"Melhores hiperparâmetros:\", best_params)\n",
    "\n",
    "# Treinando o modelo final com os melhores hiperparâmetros\n",
    "best_rf = RandomForestRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "    random_state=42\n",
    ")\n",
    "best_rf.fit(sy_x_train_scaled, sy_y_train)\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_rf.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_random_forest_model_optuna.pkl')\n",
    "joblib.dump(best_rf, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14727e8",
   "metadata": {},
   "source": [
    "## Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61e257",
   "metadata": {},
   "source": [
    "Fazendo tuning com GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2c1c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Mean Squared Error (mse) on test data: 6.18329638505496\n",
      "Modelo salvo em: Modelos\\best_Linear_Regressor_model_grid.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Faculdade\\TCC_Music\\TCC_Music\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Definindo os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Inicializando o modelo Random Forest\n",
    "lin_reg = LinearRegression() # Regressor logistico\n",
    "\n",
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Configurando o GridSearchCV com PredefinedSplit\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lin_reg,\n",
    "    param_grid={},  # Sem hiperparâmetros para ajustar\n",
    "    cv=ps,  # Passa o PredefinedSplit\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o GridSearchCV\n",
    "grid_search.fit(X_full, y_full)\n",
    "\n",
    "# Obtendo o melhor modelo\n",
    "best_lin_reg = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_lin_reg.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (mse) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_Linear_Regressor_model_grid.pkl')\n",
    "joblib.dump(best_lin_reg, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f0e7e",
   "metadata": {},
   "source": [
    "Fazendo o Tuning com Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "844bcacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-17 15:47:13,647] A new study created in memory with name: no-name-8decee07-315f-4476-8cb2-7e5be21e71fa\n",
      "[I 2025-04-17 15:47:13,671] Trial 0 finished with value: 7.789384479414533 and parameters: {}. Best is trial 0 with value: 7.789384479414533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data: 6.245837740756641\n",
      "Modelo salvo em: Modelos\\best_linear_regressor_model_optuna.pkl\n"
     ]
    }
   ],
   "source": [
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Função objetivo para o Optuna\n",
    "def objective(trial):\n",
    "    # Inicializando o modelo Linear Regression\n",
    "    lin_reg = LinearRegression()\n",
    "\n",
    "    # Treinando o modelo nos dados de treino\n",
    "    lin_reg.fit(X_full[ps.test_fold == -1], y_full[ps.test_fold == -1])\n",
    "\n",
    "    # Avaliando o modelo nos dados de validação\n",
    "    predictions = lin_reg.predict(X_full[ps.test_fold == 0])\n",
    "    mse = mean_squared_error(y_full[ps.test_fold == 0], predictions)\n",
    "\n",
    "    return mse  # O objetivo é minimizar o MSE\n",
    "\n",
    "# Criando o estudo do Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=1)  # Apenas 1 tentativa, pois não há hiperparâmetros para ajustar\n",
    "\n",
    "# Treinando o modelo final\n",
    "best_lin_reg = LinearRegression()\n",
    "best_lin_reg.fit(sy_x_train_scaled, sy_y_train)\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_lin_reg.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_linear_regressor_model_optuna.pkl')\n",
    "joblib.dump(best_lin_reg, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f81974",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedd25c",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b29891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Inicializando o modelo Random Forest\n",
    "xgb_reg = LinearRegression() # Regressor logistico\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Configurando o GridSearchCV com PredefinedSplit\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Métrica de avaliação\n",
    "    cv=ps,  # Número de folds para validação cruzada\n",
    "    verbose=3,  # Mostra o progresso do tuning\n",
    "    n_jobs=-1  # Usa todos os núcleos disponíveis\n",
    ")\n",
    "\n",
    "# Treinando o GridSearchCV\n",
    "grid_search.fit(X_full, y_full)\n",
    "\n",
    "# Obtendo o melhor modelo\n",
    "best_xgb_reg = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_xgb_reg.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (mse) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_xgb_model_grid.pkl')\n",
    "joblib.dump(best_xgb_reg, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255e2b8",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab488c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Função objetivo para o Optuna\n",
    "def objective(trial):\n",
    "    # Sugerindo os hiperparâmetros\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200, step=50)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 3, 10, step=1)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, step=0.05)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.6, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0, step=0.1)\n",
    "\n",
    "    # Criando o modelo com os hiperparâmetros sugeridos\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Treinando o modelo nos dados de treino\n",
    "    xgb_reg.fit(X_full[ps.test_fold == -1], y_full[ps.test_fold == -1])\n",
    "\n",
    "    # Avaliando o modelo nos dados de validação\n",
    "    predictions = xgb_reg.predict(X_full[ps.test_fold == 0])\n",
    "    mse = mean_squared_error(y_full[ps.test_fold == 0], predictions)\n",
    "\n",
    "    return mse  # O objetivo é minimizar o MSE\n",
    "\n",
    "# Criando o estudo do Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Número de tentativas\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = study.best_params\n",
    "print(\"Melhores hiperparâmetros:\", best_params)\n",
    "\n",
    "# Treinando o modelo final com os melhores hiperparâmetros\n",
    "best_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "best_xgb.fit(sy_x_train_scaled, sy_y_train)\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_xgb.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_xgb_model_optuna.pkl')\n",
    "joblib.dump(best_xgb, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52b921",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ceff5",
   "metadata": {},
   "source": [
    "GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Definindo os hiperparâmetros para o GridSearch\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Inicializando o modelo MLP\n",
    "mlp_reg = MLPRegressor(random_state=42, max_iter=500)\n",
    "\n",
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Configurando o GridSearchCV com PredefinedSplit\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Métrica de avaliação\n",
    "    cv=ps,  # Número de folds para validação cruzada\n",
    "    verbose=3,  # Mostra o progresso do tuning\n",
    "    n_jobs=-1  # Usa todos os núcleos disponíveis\n",
    ")\n",
    "\n",
    "# Treinando o GridSearchCV\n",
    "grid_search.fit(X_full, y_full)\n",
    "\n",
    "# Obtendo o melhor modelo\n",
    "best_mlp_reg = grid_search.best_estimator_\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_mlp_reg.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_mlp_model_grid.pkl')\n",
    "joblib.dump(best_mlp_reg, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b433f",
   "metadata": {},
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os arrays normalizados de volta para DataFrame\n",
    "sy_x_train_scaled_df = pd.DataFrame(sy_x_train_scaled, columns=sy_x_train.columns, index=sy_x_train.index)\n",
    "sy_30_x_val_scaled_df = pd.DataFrame(sy_30_x_val_scaled, columns=sy_30_x_val.columns, index=sy_30_x_val.index)\n",
    "\n",
    "# Junta treino e validação\n",
    "X_full = pd.concat([sy_x_train_scaled_df, sy_30_x_val_scaled_df])\n",
    "y_full = pd.concat([sy_y_train, sy_30_y_val])\n",
    "\n",
    "# Cria vetor de validação: -1 para treino, 0 para validação\n",
    "validation_fold = np.concatenate([\n",
    "    np.full(len(sy_x_train), -1),\n",
    "    np.zeros(len(sy_30_x_val))\n",
    "])\n",
    "\n",
    "# Cria o PredefinedSplit\n",
    "ps = PredefinedSplit(test_fold=validation_fold)\n",
    "\n",
    "# Função objetivo para o Optuna\n",
    "def objective(trial):\n",
    "    # Sugerindo os hiperparâmetros\n",
    "    hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\", [(50,), (100,), (50, 50), (100, 50)])\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"adam\", \"sgd\"])\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.0001, 0.01, log=True)\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"adaptive\"])\n",
    "\n",
    "    # Criando o modelo com os hiperparâmetros sugeridos\n",
    "    mlp_reg = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=500,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Treinando o modelo nos dados de treino\n",
    "    mlp_reg.fit(X_full[ps.test_fold == -1], y_full[ps.test_fold == -1])\n",
    "\n",
    "    # Avaliando o modelo nos dados de validação\n",
    "    predictions = mlp_reg.predict(X_full[ps.test_fold == 0])\n",
    "    mse = mean_squared_error(y_full[ps.test_fold == 0], predictions)\n",
    "\n",
    "    return mse  # O objetivo é minimizar o MSE\n",
    "\n",
    "# Criando o estudo do Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # Número de tentativas\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_params = study.best_params\n",
    "print(\"Melhores hiperparâmetros:\", best_params)\n",
    "\n",
    "# Treinando o modelo final com os melhores hiperparâmetros\n",
    "best_mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=best_params[\"hidden_layer_sizes\"],\n",
    "    activation=best_params[\"activation\"],\n",
    "    solver=best_params[\"solver\"],\n",
    "    alpha=best_params[\"alpha\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "best_mlp.fit(sy_x_train_scaled, sy_y_train)\n",
    "\n",
    "# Avaliando o modelo nos dados de teste\n",
    "predictions = best_mlp.predict(sy_30_x_test_scaled)\n",
    "mse = mean_squared_error(sy_30_y_test, predictions)\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n",
    "\n",
    "# Salvando o modelo na pasta 'Modelos'\n",
    "os.makedirs('Modelos', exist_ok=True)\n",
    "model_path = os.path.join('Modelos', 'best_mlp_model_optuna.pkl')\n",
    "joblib.dump(best_mlp, model_path)\n",
    "print(f\"Modelo salvo em: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
